
ReLU (Rectified Linear Unit) is a widely used activation function which outputs the input directly if it's positive and replaces any negative value with zero.

$f(x) = \max(0,x)$

The purpose of conveting negative values to zero is to introduces non-linearity and improve the performance of deep learning models.

